{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "53c6f68f-e87b-4e7b-803b-c239f2398457",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "\n",
    "1. Read .docx files using python-docx\n",
    "2. Split text into chunks using LangChain.TextSplitter\n",
    "3. Generate embeddings using HuggingFaceEmbeddings with sentence-transformers/all_miniLM-L6-v2\n",
    "4. Store and retrieve embeddings using FAISS\n",
    "\n",
    "## Environment\n",
    "\n",
    "Created a venv using Python 3.9.23.\n",
    "\n",
    "To access the venv:\n",
    "\n",
    "1. `cd \"AI SOP Assistant\"`\n",
    "2. `source venv/bin/activate`\n",
    "\n",
    "### Dependencies\n",
    "\n",
    "- currently using Python version 3.9\n",
    "- NumPy version must be >1.21, <2 (currently using 1.26.4)\n",
    "- torch must be >=1.0.1 for sentence-transformers\n",
    "- python-docx, langchain, sentence-transformers, faiss-cpu, -U langchain-community\n",
    "- for more details, see requirements.txt file\n",
    "- installed via pip in Unix command line: `pip install ...`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de4c04b-6ab2-437d-a3c8-a2e203aa2560",
   "metadata": {},
   "source": [
    "## Code\n",
    "### Library Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "14307923-78d2-45d4-be60-054e2c3b41ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing libraries\n",
    "import os\n",
    "from docx import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.schema import Document as LCDocument"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd935fa-0a53-4b25-b58e-6bfbb6c0393b",
   "metadata": {},
   "source": [
    "### Loading and Reading Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7869281-5a06-43b6-b686-7bbdbb07e76a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Folder where the .docx SOP files are stored\n",
    "folder_path='SOPs'\n",
    "#List to store text and metadata from each document\n",
    "docs=[]\n",
    "\n",
    "#Creating a loop that goes through every file in the SOPs folder\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith('.docx'):\n",
    "        full_path=os.path.join(folder_path,filename) #Getting full path of the file\n",
    "        doc = Document(full_path) #Opens the Word document\n",
    "        #Joining all non-empty paragraphs into one string\n",
    "        full_text='\\n'.join([para.text for para in doc.paragraphs if para.text.strip() != \"\"])\n",
    "        #Storing the text along with its filename as metadata\n",
    "        docs.append({\"text\":full_text,\"source\":filename})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5a2e5ea6-ec5a-4ea2-b2db-bb4ef2f4a4ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total documents: 26\n",
      "Filename: SOP Termination of Employee.docx\n",
      "Termination of Employee SOP\n",
      "Purpose\n",
      "To define a secure, standardized process for handling IT tasks associated with employee termination. This ensures timely revocation of access, data protection, and operational continuity in compliance with company policy and data security standards. This procedure applies to all El Sol NEC employees, contractors, interns, and external users whose employment or engagement has ended, either voluntarily or involuntarily.\n",
      "Pre-Termination (For Planned Separations Only)\n",
      "Inventory all devices assigned to the user, review active accounts, and list any shared access or privileged roles.\n",
      "Open an internal IT ticket with the due date/time to match the employeeâ€™s termination date/time.\n",
      "Day of Termination\n",
      "Immediately revoke access exactly at termination time.\n",
      "Disable Microsoft 365 account\n",
      "Terminate VPN access and revoke tokens\n",
      "Disable access to internal systems, SaaS platforms, and admin panels.\n",
      "Disable access to mobile device management tools\n",
      "Revoke SSO credentia\n"
     ]
    }
   ],
   "source": [
    "#Total number of documents read\n",
    "print(f\"Total documents: {len(docs)}\")\n",
    "\n",
    "#Preview of the first document\n",
    "print(f\"Filename: {docs[0]['source']}\")\n",
    "print(docs[0]['text'][:1000])  #First 1000 characters only"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b91684-279a-471b-b86b-d3a596a02bb6",
   "metadata": {},
   "source": [
    "### Splitting Text into Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d88a168e-9beb-49c9-8ed6-0111d1c8457b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting up the text splitter to break text into chunks of ~500 characters with some overlap (~100 characters)\n",
    "text_splitter=RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500, #Max size\n",
    "    chunk_overlap=100, #Max overlap to preserve context\n",
    "    separators=[\"\\n\\n\",\"\\n\",\".\",\" \"] #Preferred places to split text\n",
    ")\n",
    "\n",
    "#List of final chunked documents\n",
    "documents=[]\n",
    "\n",
    "#For loop to chunk text for each SOP\n",
    "for d in docs:\n",
    "    #Splitting text into chunks\n",
    "    chunks=text_splitter.split_text(d['text'])\n",
    "    #Creating LangChain document with metadata\n",
    "    for chunk in chunks:\n",
    "        documents.append(LCDocument(page_content=chunk,metadata={\"source\":d[\"source\"]}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1e458921-9e0d-44a6-8347-7b6781979f59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average chunk length: 402.31 characters\n"
     ]
    }
   ],
   "source": [
    "chunk_lengths=[len(doc.page_content) for doc in documents]\n",
    "avg_len=sum(chunk_lengths)/len(chunk_lengths)\n",
    "print(f\"Average chunk length: {avg_len:.2f} characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f2c2e3-4977-4e42-b555-4a12065bf2ed",
   "metadata": {},
   "source": [
    "### Generating Embeddings\n",
    "\n",
    "An embedding model converts each chunk of text into a numerical vector, called an embedding, that captures its meaning. Vector lengths should be 384 for `all-MiniLM-L6-v2`. When generating embeddings, we don't need to set a seed because embedding models are deterministic during inference. This means that  results are always the same for a given input text and model version (no randomness involved). May need to set seed when fine-tuning the embedding model, doing train/text splits for ML tasks, random sampling of chunks, or using LLMs that involve sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "48ab7e07-6abf-43a4-89fb-5578076e1c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading a sentence embedding model from Hugging Face\n",
    "embedding_model=HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc98d39-adb6-4b75-8505-357051c2ffa0",
   "metadata": {},
   "source": [
    "### Storing Embeddings in FAISS\n",
    "\n",
    "Storing the FAISS index locally for reuse allows it to be loaded later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ffc78697-0aaf-4270-81d0-801c67637fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a FAISSS vector index from list of documents and their embeddings\n",
    "vectorstore=FAISS.from_documents(documents,embedding_model)\n",
    "#Saving the FAISS index locally\n",
    "vectorstore.save_local(\"faiss_sop_index\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e5451f4-08cb-42db-bff3-01ae5e059362",
   "metadata": {},
   "source": [
    "Below, we ran an example to check the embeddings of the SOPs. This is to confirm that embeddings were generated, FAISS was able to store them, and that semantic similarity search works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8dc857ec-b6b9-457d-931a-ef117f83d350",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Match 1\n",
      "Source: SOP Asset Management and Inventory Tracking.docx\n",
      "Store purchase and warranty documentation in a secure, access-controlled folder.\n",
      "Follow the Device Setup and Configuration SOP to configure and assign devices. Ensure the asset record is updated with deployment date, assigned user name and department, technician name, and setup confirmation. Require\n",
      "\n",
      "Match 2\n",
      "Source: SOP Device Setup and Configuration.docx\n",
      "Document successful setup in the ticketing system and close the setup ticket.\n",
      "Provide user with device, power supply, and other peripherals, deliver a quick orientation, and require signature on Device Receipt Acknowledgement Form.\n",
      "Update inventory records with assigned user name and email, device s\n",
      "\n",
      "Match 3\n",
      "Source: SOP Hardware Repair and Replacement.docx\n",
      "Upon return, inspect and test the device thoroughly. Reconfigure and setup the device as needed (see SOP on Device Setup and Configuration).\n",
      "Replacement Procedure\n",
      "If the device is beyond repair or unserviceable, document approval from IT Lead, select replacement model from pre-approved hardware list\n"
     ]
    }
   ],
   "source": [
    "#Recreating FAISS index temporarily in memory\n",
    "vectorstore=FAISS.from_documents(documents,embedding_model)\n",
    "\n",
    "#Running a test similarity search with max 3 sources\n",
    "results=vectorstore.similarity_search(\"How to set up a device?\",k=3)\n",
    "\n",
    "#Printing results (match #, source SOP, and snippet of 300 characters)\n",
    "for i, doc in enumerate(results):\n",
    "    print(f\"\\nMatch {i+1}\")\n",
    "    print(f\"Source: {doc.metadata['source']}\")\n",
    "    print(doc.page_content[:300])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5810a908-dda0-4a58-9cd6-7ec729e20d6c",
   "metadata": {},
   "source": [
    "### Balancing Chunk Size and Semantic Precision\n",
    "\n",
    "Pros of smaller chunks:\n",
    "\n",
    "- Smaller chunks are less likely to contain multiple topics, which makes their embeddings more semantically precise\n",
    "- The search query is more likely to find a chunk that directly addresses it\n",
    "- Core meaning isn't averaged out across unrelated content\n",
    "\n",
    "Cons of smaller chunks:\n",
    "\n",
    "- Small chunks <100 characters can miss necessary surrounding info\n",
    "- More chunks = more embeddings = bigger vector index and slower searches (unless optimized)\n",
    "- Some procedures or instructions can get split mid-thought, reducing interpretability on retrieval\n",
    "\n",
    "Smaller chunks generally lead to more accurate semantic similarity, but going too small might hurt performance by cutting off useful context. Sweet spot for SOPs is often a chunk size of 500 characters with an overlap of 100 characters.\n",
    "\n",
    "https://www.reddit.com/r/LangChain/comments/1bgqc2o/optimal_way_to_chunk_word_document_for/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f0c94b-6ebc-497f-8885-3816fcdc403d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9 (sop_env)",
   "language": "python",
   "name": "sop_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
